{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code below is meant to perform a loop over the webpage and each time scrap data from the table. For some reason not yet figured out, it doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "pagenum = 0\n",
    "all_data = []\n",
    "with requests.Session() as s:\n",
    "    s.headers = {'User-Agent':'python-requests/2.28.2'}\n",
    "    requests.session().headers\n",
    "    while True:\n",
    "        # URL of the web page containing the HTML table to scrape\n",
    "        #url = 'https://hfr.health.gov.ng/facilities/hospitals-list?page=' + str(i)\n",
    "        pagenum+=1\n",
    "        url = 'https://hfr.health.gov.ng/facilities/hospitals-search?_token=lqDSClMX3WVEbiQnIbJfYQKYUajok3Y1WymK3E2l&state_id=1&lga_id=1&ward_id=0&facility_level_id=0&ownership_id=0&operational_status_id=1&registration_status_id=0&license_status_id=0&geo_codes=0&service_type=0&service_category_id=0&entries_per_page=500&page={pagenum}'\n",
    "        # Fetch the web page content using requests library\n",
    "        page = s.get(url).text\n",
    "\n",
    "        # Create BeautifulSoup object from HTML\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        # Find the table element in the HTML\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Get the column names from the table header\n",
    "        headers = []\n",
    "        for th in table.find_all('th'):\n",
    "            headers.append(th.text.strip().replace(\" \",\"_\").lower())\n",
    "\n",
    "        # Loop over each row in the table body and store the data in a list of dictionaries\n",
    "        data = []\n",
    "        for tr in table.find_all('tr'):\n",
    "            row = {}\n",
    "            for i, td in enumerate(tr.find_all('td')):\n",
    "                row[headers[i]] = td.text.strip()\n",
    "            if row:\n",
    "                data.append(row)\n",
    "        \n",
    "        # Append the data from the current page to the list of all data\n",
    "        all_data.extend(data)\n",
    "\n",
    "        # Find the next page link (if any)\n",
    "        next_link = soup.find('a', {'class': 'page-link'},True)\n",
    "   \n",
    "    \n",
    "        if next_link:\n",
    "            # If there is a next link, update the URL and continue to next page\n",
    "            url = next_link['href']\n",
    "        else:\n",
    "            # If there is no next link, we're done scraping\n",
    "            break\n",
    "   \n",
    "# Create pandas DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(all_data)\n",
    "print(df)\n",
    "\n",
    "#df.to_excel(\"bystate.xlsx\",sheet_name=\"Hospital number\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code showing to create new columns on a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a dictionary of films\n",
    "films = {\n",
    "    \"Id\": [1,2,3,5,6],\n",
    "    \"Title\": [\"Jurassic Park\", \"Spider-man\", \"King-Kong\", \"Superman Returns\",\"Titanic\"],\n",
    "    \"Released Date\": [\"1993-06-11\",\"2002-05-03\",\"2005-12-14\",\"2006-07-14\",\"1998-01-23\"],\n",
    "    \"Run Time\": [126, 121, 187,154, 194],\n",
    "    \"Genre\": [\"Adventure\", \"Action\", \"Adventure\", \"Action\", \"Romance\"]\n",
    "}\n",
    "\n",
    "# create a Pandas dataframe based upon this\n",
    "df = pd.DataFrame(films)\n",
    "\n",
    "# lenght of film title (the new column on the left = some calculation done on the old df)\n",
    "df[\"Title length\"] = df[\"Title\"].str.len()\n",
    "\n",
    "# hours and minutes\n",
    "df[\"Hours\"] = np.floor(df[\"Run Time\"] / 60).astype(int)\n",
    "df[\"Minutes\"] = df[\"Run Time\"] % 60\n",
    "\n",
    "# convert the \"date\" to a real date\n",
    "df[\"RealDate\"] = pd.to_datetime(df[\"Released Date\"])\n",
    "df[\"RelYear\"] = pd.DatetimeIndex(df[\"RealDate\"]).year\n",
    "\n",
    "# print this out\n",
    "print(\"\\n\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to parse a HTML table into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the website and the filter parameter\n",
    "url = 'https://hfr.health.gov.ng/facilities/hospitals-list'\n",
    "params = {'filter': 'Tertiary'}\n",
    "\n",
    "# Send a GET request to the website with the filter parameter\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table on the website using its HTML tag and class\n",
    "table = soup.find('table')\n",
    "\n",
    "# Convert the table into a Pandas DataFrame\n",
    "df = pd.read_html(str(table))[0]\n",
    "print(df)\n",
    "# Save the DataFrame to an Excel workbook\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
